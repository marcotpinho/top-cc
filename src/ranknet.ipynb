{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44986782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from db_utils import load_from_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e320aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankNetDataset(Dataset):\n",
    "    def __init__(self, data_dir=\"../data\", split=\"train\"):\n",
    "        data_file = f\"{data_dir}/distances_{split}_.json\"\n",
    "        print(f\"Loading data from {data_file}...\")\n",
    "\n",
    "        self.split = split\n",
    "        self.X, self.y, self.map_groups = load_from_db(data_file)\n",
    "\n",
    "        with open(f\"{data_dir}/speeds_stats.json\", 'r') as f:\n",
    "            stats = json.load(f)\n",
    "            self.std_speed = stats[\"std_speed\"]\n",
    "            self.mean_speed = stats[\"mean_speed\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class PairDataset(RankNetDataset):\n",
    "    def __init__(self, same_map_only=False, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.same_map_only = same_map_only\n",
    "\n",
    "    def __getitem__(self, i1):\n",
    "        i1_x, i1_y = super().__getitem__(i1)\n",
    "\n",
    "        if self.same_map_only:\n",
    "            i2 = np.random.choice(self.map_groups[i1_x[\"map\"]])\n",
    "        else:\n",
    "            i2 = np.random.randint(len(self))\n",
    "\n",
    "        i2_x, i2_y = super().__getitem__(i2)\n",
    "\n",
    "        speeds1, speeds2 = self.normalize_speeds(i1_x[\"speeds\"], i2_x[\"speeds\"])\n",
    "        coords1 = self.normalize_coordinates(\n",
    "            i1_x[\"coordinates\"],\n",
    "            i1_x[\"map_bounds\"][\"max_x\"], i1_x[\"map_bounds\"][\"max_y\"],\n",
    "            i1_x[\"map_bounds\"][\"min_x\"], i1_x[\"map_bounds\"][\"min_y\"]\n",
    "        )\n",
    "        coords2 = self.normalize_coordinates(\n",
    "            i2_x[\"coordinates\"],\n",
    "            i2_x[\"map_bounds\"][\"max_x\"], i2_x[\"map_bounds\"][\"max_y\"],\n",
    "            i2_x[\"map_bounds\"][\"min_x\"], i2_x[\"map_bounds\"][\"min_y\"]\n",
    "        )\n",
    "        # ts1 = self.normalize_timestamps(i1_x[\"timestamps\"])\n",
    "        # ts2 = self.normalize_timestamps(i2_x[\"timestamps\"])\n",
    "        # coords1 = [np.concatenate([c, t[:, None]], axis=1) for c, t in zip(coords1, ts1)]\n",
    "        # coords2 = [np.concatenate([c, t[:, None]], axis=1) for c, t in zip(coords2, ts2)]\n",
    "\n",
    "        if i1_y > i2_y:\n",
    "            label = 1\n",
    "        elif i1_y < i2_y:\n",
    "            label = -1\n",
    "        else:\n",
    "            label = 0\n",
    "\n",
    "        return (coords1, speeds1), (coords2, speeds2), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "    def normalize_speeds(self, speeds1, speeds2):\n",
    "        if self.std_speed > 0:\n",
    "            return (speeds1 - self.mean_speed) / self.std_speed, (speeds2 - self.mean_speed) / self.std_speed\n",
    "        return speeds1 - self.mean_speed, speeds2 - self.mean_speed\n",
    "\n",
    "    def normalize_coordinates(self, coords, max_x, max_y, min_x, min_y):\n",
    "        max_vals = np.array([max_x, max_y])\n",
    "        min_vals = np.array([min_x, min_y])\n",
    "        map_diag = np.linalg.norm(max_vals - min_vals)\n",
    "        center = (max_vals + min_vals) / 2\n",
    "        coords_normalized = []\n",
    "        for c in coords:\n",
    "            coords_normalized.append((c - center) / (map_diag + 1e-6))\n",
    "        return coords_normalized\n",
    "\n",
    "    def normalize_timestamps(self, timestamps: list) -> np.ndarray:\n",
    "        times_flat = np.array([ts for times in timestamps for ts in times])\n",
    "        t_max = times_flat.max()\n",
    "        t_min = times_flat.min()\n",
    "        times_normalized = []\n",
    "        for ts in timestamps:\n",
    "            times_normalized.append((ts - t_min) / (t_max - t_min + 1e-6))\n",
    "        return times_normalized\n",
    "\n",
    "class RankNetDataloader(DataLoader):\n",
    "    def __init__(self, dataset, *args, **kwargs):\n",
    "        super().__init__(dataset, collate_fn=self.collate_fn, *args, **kwargs)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        xs = []\n",
    "        x_speeds = []\n",
    "        x_len = []\n",
    "        ids = []\n",
    "        for id, x in enumerate(batch):\n",
    "            coords, speeds = x\n",
    "            xs.extend([torch.tensor(path, dtype=torch.float32) for path in coords])\n",
    "            x_speeds.extend(speeds)\n",
    "            x_len.extend([len(path) for path in coords])\n",
    "            ids.extend([id] * len(coords))\n",
    "\n",
    "        padded_x = nn.utils.rnn.pad_sequence(xs, batch_first=True)\n",
    "        x_speeds = torch.tensor(x_speeds, dtype=torch.float32)\n",
    "        x_len = torch.tensor(x_len, dtype=torch.long)\n",
    "        ids = torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "        return padded_x, x_speeds, x_len, ids\n",
    "\n",
    "class PairDataloader(RankNetDataloader):\n",
    "    def collate_fn(self, batch):\n",
    "        x1, x2, labels = zip(*batch)\n",
    "        padded_x1, speeds1, x1_len, ids1 = super().collate_fn(x1)\n",
    "        padded_x2, speeds2, x2_len, ids2 = super().collate_fn(x2)\n",
    "        labels = torch.stack(labels)\n",
    "        return (padded_x1, speeds1, x1_len, ids1), (padded_x2, speeds2, x2_len, ids2), labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76234d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../data/distances_train_.json...\n"
     ]
    }
   ],
   "source": [
    "train_dataset = PairDataset(split=\"train\", same_map_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24e6712f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../data/distances_val_.json...\n"
     ]
    }
   ],
   "source": [
    "val_dataset = PairDataset(split=\"val\", same_map_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc8d3fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankNet(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.phi = Encoder(input_dim=input_dim, hidden_dim=hidden_dim)\n",
    "        self.agent_emb = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + 1, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.att = InteractionModule(hidden_dim=hidden_dim)\n",
    "        self.rho = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, padded_x, speeds, x_len, x_ids):\n",
    "        encoded_x = self.phi(padded_x, x_len)\n",
    "        emb = torch.cat([encoded_x, speeds.unsqueeze(1)], dim=1)\n",
    "        encoded_x = self.agent_emb(emb)\n",
    "\n",
    "        unique_ids, inverse_indices = torch.unique(x_ids, return_inverse=True)\n",
    "        num_unique = len(unique_ids)\n",
    "\n",
    "        summed = torch.zeros(num_unique, encoded_x.size(1), device=encoded_x.device)\n",
    "        summed.index_add_(0, inverse_indices, encoded_x)\n",
    "        # summed = self.att(encoded_x, x_ids)\n",
    "\n",
    "        scores = self.rho(summed)\n",
    "\n",
    "        return scores\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, num_layers=2)\n",
    "    \n",
    "    def forward(self, padded_x, lengths):\n",
    "        packed_x = nn.utils.rnn.pack_padded_sequence(padded_x,\n",
    "                                                     lengths=lengths,\n",
    "                                                     batch_first=True,\n",
    "                                                     enforce_sorted=False)\n",
    "        _, (h_n, _) = self.lstm(packed_x)\n",
    "        ordered_h_n = h_n.index_select(1, packed_x.unsorted_indices)\n",
    "        return ordered_h_n[-1]\n",
    "\n",
    "class InteractionModule(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, nhead=4, nlayers=2):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=nlayers)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, agents_embs, ids):\n",
    "        unique_ids, idx = ids.unique(return_inverse=True)\n",
    "        num_groups = unique_ids.shape[0]\n",
    "        group_sizes = torch.bincount(idx)\n",
    "        max_group_size = group_sizes.max()\n",
    "\n",
    "        group_positions = torch.zeros_like(idx, device=agents_embs.device)\n",
    "        group_positions[torch.argsort(idx)] = torch.cat([torch.arange(size, device=agents_embs.device) for size in group_sizes])\n",
    "\n",
    "        flat_indices = idx * max_group_size + group_positions\n",
    "        group_embs = torch.zeros(num_groups * max_group_size, self.hidden_dim, device=agents_embs.device)\n",
    "        padding_mask = torch.ones(num_groups * max_group_size, dtype=torch.bool, device=agents_embs.device)\n",
    "    \n",
    "        group_embs.scatter_(0, flat_indices.unsqueeze(1).expand_as(agents_embs), agents_embs)\n",
    "        padding_mask.scatter_(0, flat_indices, False)\n",
    "        \n",
    "        group_embs = group_embs.view(num_groups, max_group_size, self.hidden_dim)\n",
    "        padding_mask = padding_mask.view(num_groups, max_group_size)\n",
    "        \n",
    "        transformer_output = self.transformer(group_embs, src_key_padding_mask=padding_mask)\n",
    "        \n",
    "        masked_output = transformer_output * (~padding_mask).unsqueeze(-1).float()\n",
    "        valid_lengths = (~padding_mask).sum(dim=1, keepdim=True).float()  \n",
    "        pooled_outputs = masked_output.sum(dim=1) / torch.clamp(valid_lengths, min=1.0)\n",
    "        return pooled_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73eb4c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, (x1, x2, labels) in enumerate(tqdm(loader)):\n",
    "        padded_x1, speeds1, x1_len, x1_ids = x1\n",
    "        padded_x1, speeds1, x1_ids = padded_x1.to(device), speeds1.to(device), x1_ids.to(device)\n",
    "        x1_len = x1_len.cpu()\n",
    "\n",
    "        padded_x2, speeds2, x2_len, x2_ids = x2\n",
    "        padded_x2, speeds2, x2_ids = padded_x2.to(device), speeds2.to(device), x2_ids.to(device)\n",
    "        x2_len = x2_len.cpu()\n",
    "\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        s1 = model(padded_x1, speeds1, x1_len, x1_ids)\n",
    "        s2 = model(padded_x2, speeds2, x2_len, x2_ids)\n",
    "        if i == 0:\n",
    "            print(f\"\\n=== DIAGNOSTICS ===\")\n",
    "            print(f\"s1: min={s1.min():.6f}, max={s1.max():.6f}, mean={s1.mean():.6f}, std={s1.std():.6f}\")\n",
    "            print(f\"s2: min={s2.min():.6f}, max={s2.max():.6f}, mean={s2.mean():.6f}, std={s2.std():.6f}\")\n",
    "            print(f\"s1-s2: min={(s1-s2).min():.6f}, max={(s1-s2).max():.6f}, mean={(s1-s2).mean():.6f}\")\n",
    "            print(f\"Labels: {labels[:10]}\")  # First 10 labels\n",
    "\n",
    "        loss = criterion(s1, s2, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i == 0:\n",
    "            # Check if gradients exist\n",
    "            grad_norm = sum(p.grad.norm().item()**2 for p in model.parameters() if p.grad is not None)**0.5\n",
    "            print(f\"Gradient norm: {grad_norm:.6f}\")\n",
    "            \n",
    "            # Check specific layer gradients\n",
    "            print(f\"LSTM grad norm: {model.phi.lstm.weight_ih_l0.grad.norm():.6f}\")\n",
    "            print(f\"MLP final layer grad norm: {model.rho[-1].weight.grad.norm():.6f}\")\n",
    "            print(\"==================\\n\")\n",
    "\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, labels in tqdm(loader):\n",
    "            padded_x1, speeds1, x1_len, x1_ids = x1\n",
    "            padded_x1, speeds1, x1_ids = padded_x1.to(device), speeds1.to(device), x1_ids.to(device)\n",
    "\n",
    "            padded_x2, speeds2, x2_len, x2_ids = x2\n",
    "            padded_x2, speeds2, x2_ids = padded_x2.to(device), speeds2.to(device), x2_ids.to(device)\n",
    "\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            s1 = model(padded_x1, speeds1, x1_len, x1_ids)\n",
    "            s2 = model(padded_x2, speeds2, x2_len, x2_ids)\n",
    "\n",
    "            loss = criterion(s1, s2, labels)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, device, criterion):\n",
    "    losses = []\n",
    "    patience = 15\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(100):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss = evaluate(model, val_loader, criterion, device)\n",
    "        losses.append((train_loss, val_loss))\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            patience = 15\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience == 0:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss}, Val Loss = {val_loss}\")\n",
    "    return losses\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight, gain=1.0)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "\n"
  ]
}
</VSCode.Cell>
<VSCode.Cell id="#VSC-39a1bcee" language="python">
{
  "text": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = RankNet(input_dim=2, hidden_dim=128).to(device)\n",
    "model.apply(init_weights)\n",
    "train_loader = PairDataloader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = PairDataloader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "loss_fn = torch.nn.MarginRankingLoss(margin=0.1)\n",
    "# loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "def ranknet_loss(s1, s2, t, loss_fn=loss_fn):\n",
    "    non_tie = (t != 0)\n",
    "    s1_masked = s1.squeeze()[non_tie]\n",
    "    s2_masked = s2.squeeze()[non_tie]\n",
    "    t_masked = t[non_tie]\n",
    "    return loss_fn(s1_masked, s2_masked, t_masked)\n"
  ]
}
</VSCode.Cell>
<VSCode.Cell id="#VSC-2f14bad2" language="python">
{
  "text": [
    "train(model, train_loader, val_loader, optimizer, device, ranknet_loss)\n"
  ]
}
</VSCode.Cell>
<VSCode.Cell id="#VSC-ade47421" language="python">
{
  "text": [
    "labels_count = {0: 0, 0.5: 0, 1: 0}\n",
    "for i in range(len(train_dataset)):\n",
    "    _, _, label = train_dataset[i]\n",
    "    if label < 0.3:\n",
    "        labels_count[0] += 1\n",
    "    elif label > 0.7:\n",
    "        labels_count[1] += 1\n",
    "    else:\n",
    "        labels_count[0.5] += 1\n",
    "print(labels_count)\n"
  ]
}
</VSCode.Cell>
